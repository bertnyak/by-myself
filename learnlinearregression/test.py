import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt

# Данные
# Создаем массив х из входных данных (независимая переменная)
# .reshape(-1, 1) преобразует одномерный массив в двумерный столбец (5 строк 1 столбец)
# так как scikit-learn ожидает массив для входных данных
x = np.array([2, 3, 5, 7, 9]).reshape(-1, 1)

# создаем массив y из целевых значений(зависимая переменная)
# у остается одномерным6 так как это ожидаемый формат для целевых значений
y = np.array([4, 6, 8, 10, 12])

# Создае и обучаем модель линейной регрессии
# LinearRegression() создаем объект модели линейной регрессии
# Этот класс реализует метод наименьших квадратов (OLS) для нахождения оптимальных параметров
model = LinearRegression()

# Метод fit обучает модель на данных
# Он принимает х (матрица признаков) и у (целевые значения) и вычисляет коэфф регрессии
# внутри fit использует аналитическое решение (нормальное уравнение :
# w =(X^T X)^(-1) X^T y)
model.fit(x, y)

"""
Получаем коэффициенты модели
model.coef_ возвращаает коэффициенты (наклом m) ддля каждого признака
Так как у нас был один признак(парная регрессия), берем первый элемент [0]
m - это наклон прямой (slope), который показывает, насколько у изменяется
при изменении х на еденицу
"""
m = model.coef_[0]
"""
model.intercept_ возвращает свободный член (intercept b)
b - это значение у , когда х = 0, тоесть точка пересечения прямой с осью у
"""
b = model.intercept_

# Выводим уравнение прямой в формате y = mx + b
# :.2f форматирует числа до 2 знаков после запятой для удобства чтения
print(f"Уравнение прямой: y = {m:.2f}x + {b:.2f}")

#Предсказания метод predict принимает входные данные х и возвращает предсказаныне значения у
# y_pred это значение, которые модель предсказывает для каждого х, испульзя уравнение y = mx+ b
y_pred = model.predict(x)

# выводим предсказанные значения для проверки
print(f"Предсказанные значения: {y_pred}")

"""
Вычичисляем средне квадратичную ошибку(MSE)
mean_squared_error сравнивает истинные значения у с предсказанными y_pred
MSE = (1/n) * sum((y_1 - y_pred)^2) где n кол-во наблюдений
Это мера качества модели: чем меньше MSE, тем лучше модель соответствует данным
"""
mse = mean_squared_error(y, y_pred)

# выводим округленное до 2 знаков после запятой
print(f"Среднеквадратичная ошибка (MSE): {mse:.2f}")

"""
Визуализация результатов
scatter создает точечный график: синие точки это исходные данные (x, y)
color = 'blue' задает цвет точек, label = 'Данные' подпись для легенды

"""
plt.scatter(x, y, color ="blue", label ="Данные")

# plot создает линию: красная линия - это предсказанные значения (x, y_pred)
plt.plot(x, y_pred, color = "red", label ="Линейная регрессия")

plt.xlabel('x')
plt.ylabel('y')
plt.title('Парная линейная регрессия')
plt.legend()
plt.show()